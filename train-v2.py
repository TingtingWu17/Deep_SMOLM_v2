import torch
from data_loader.data_loader_v2 import MicroscopyDataLoader


# CUDA for PyTorch
use_cuda = torch.cuda.is_available()
device = torch.device("cuda:0" if use_cuda else "cpu")
torch.backends.cudnn.benchmark = True

# Parameters
setup_params = {'batch_size': 64,
          'shuffle': True,
          'num_workers': 6}

max_epochs = 100

# Datasets
partition = # IDs
labels = # Labels

# Generators
training_set = Dataset(partition['train'], labels)
training_generator = torch.utils.data.DataLoader(training_set, **params)

validation_set = Dataset(partition['validation'], labels)
validation_generator = torch.utils.data.DataLoader(validation_set, **params)

# Loop over epochs
for epoch in range(max_epochs):
    # Training
    for local_batch, local_labels in training_generator:
        # Transfer to GPU
        local_batch, local_labels = local_batch.to(device), local_labels.to(device)

        # Model computations
        [...]

    # Validation
    with torch.set_grad_enabled(False):
        for local_batch, local_labels in validation_generator:
            # Transfer to GPU
            local_batch, local_labels = local_batch.to(device), local_labels.to(device)

            # Model computations
            [...]
            
            
            
            
def train_model(setup_params):
    
    # set random number generators for repeatability
    torch.manual_seed(999)
    np.random.seed(526)
    
    #setup GPU if avaiable
    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda:0" if use_cuda else "cpu")
    torch.backends.cudnn.benchmark = True

    # read the file list
    file_combo =  os.listdir(filefolder_combo)
    
    # training data folder for loading
    path_train = setup_params['training_data_path']

    # output folder for results
    path_save = setup_params['results_path']
    if not(os.path.isdir(path_save)):
        os.mkdir(path_save)

    # save setup parameters in the results folder as well
    path_setup_params = path_save + 'setup_params.pickle'
    with open(path_setup_params, 'wb') as handle:
        pickle.dump(setup_params, handle, protocol=pickle.HIGHEST_PROTOCOL)

    # open all locations pickle file
    path_pickle = path_train + 'labels.pickle'
    with open(path_pickle, 'rb') as handle:
        labels = pickle.load(handle)
    
    # Parameters for data loaders
    params_train = {'batch_size': setup_params['batch_size'], 'shuffle': True}
    params_valid = {'batch_size': setup_params['batch_size'], 'shuffle': False}

    # instantiate the data class and create a datalaoder for training
    partition = setup_params['partition']
    training_set = ImagesDataset(path_train, partition['train'], labels, setup_params)
    training_generator = DataLoader(training_set, **params_train)

    # instantiate the data class and create a datalaoder for validation
    validation_set = ImagesDataset(path_train, partition['valid'], labels, setup_params)
    validation_generator = DataLoader(validation_set, **params_valid)
    
    
    
    
    # calculate the total batches and set part of them as training batches and the other as validation batches.
    file_num = int(len(file_combo))
    training_ratio = 0.7
    training_num = int(file_num*training_ratio)
    
    print('Number of Training Examples: %d' % training_num)
    print('Number of Validation Examples: %d' % (file_num - training_num))
    
    file_combo_training = file_combo[0:training_num]
    file_combo_validation = file_combo[training_num:file_num]
    
    early_stop = EarlyStopping(monitor='val_loss',min_delta=0,patience=5)
    # Save the model weights after each epoch if the validation loss decreased
    checkpointer = ModelCheckpoint(filepath=weights_name, verbose=1,save_best_only=True)

    # Change learning when loss reaches a plataeu
    change_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.00005)

    psize =  480
    num_input_channels = 2
    model = buildModel((psize, psize,num_input_channels)) # 256*256*2 input size

    batch_size = 2
    # Create an image data generator for real time data augmentation
    my_training_batch_generator = data_Generator(file_combo_training, batch_size, filefolder_combo)
    my_validation_batch_generator = data_Generator(file_combo_validation, batch_size, filefolder_combo)
    #print('shape',np.shape(my_training_batch_generator))
    

    

    # loss history recorder
    history = LossHistory()

    # Inform user training begun
    print('Training model...')
    
    # Fit model on the batches generated by datagen.flow()
    train_history = model.fit_generator(generator = my_training_batch_generator, \
                                        steps_per_epoch = training_num, epochs=60,verbose=1, \
                                        validation_data=my_validation_batch_generator,validation_steps = file_num - training_num, \
                                        use_multiprocessing = True, \
                                        callbacks=[early_stop, history, checkpointer, change_lr])     

    # Inform user training ended
    print('Training Completed!')
    

    # plot the loss function progression during training
    loss = train_history.history['loss']
    val_loss = train_history.history['val_loss']
    
    sio.savemat(loss_fn_name,{"train_loss": loss, "val_loss": val_loss})


    return
    
if __name__ == '__main__':
    
    # start a parser
    parser = argparse.ArgumentParser()

    # path of the training data: patches and heatmaps, created in MATLAB using
    # the function "GenerateTrainingExamples.m"
    
    # parser.add_argument('--filename_xy_denoised', type=str, help="path to generated xy_denoised data m-file")
    parser.add_argument('--filefolder_combo', type=str, help="path to generated xy data m-file")
    
    
    # path for saving the optimal model weights and normalization factors after 
    # training with the function "train_model.py" is completed.
    parser.add_argument('--weights_name', type=str, help="path to save model weights as hdf5-file")
    # parser.add_argument('--meanstd_name', type=str, help="path to save normalization factors as m-file")
    parser.add_argument('--loss_fn_name', type=str, help="path to save loss curves as m-file")
    # parse the input arguments
    args = parser.parse_args()

    # run the training process
    #train_model(abspath(args.filename), abspath(args.weights_name), abspath(args.meanstd_name))
    train_model((args.filefolder_combo),(args.weights_name),(args.loss_fn_name))